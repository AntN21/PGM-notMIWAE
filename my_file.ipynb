{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General case:\n",
    "- Choose a prior for $Z$: $p(Z)$.\n",
    "- Choose an observation model: $p_\\theta(X|Z)$\n",
    "- Choose a variational posterior: $q_{\\gamma}(\\mathbf{z} | \\mathbf{x})$\n",
    "\n",
    "- Choose a missing model: $p_{\\phi}(\\mathbf{S} | \\mathbf{X^o, X^m})$\n",
    "\n",
    "\n",
    "The ELBO in the MNAR case is\n",
    "\n",
    "$$ E_{(\\mathbf{z}_1, \\mathbf{x}_1^m)...(\\mathbf{z}_K, \\mathbf{x}_K^m)} \\left[ \\log \\frac{1}{K} \\sum_{k=1}^K \\frac{p_{\\phi}(\\mathbf{s} | \\mathbf{x}^o, \\mathbf{x}_k^m) p_{\\theta}(\\mathbf{x}^o | \\mathbf{z}_k) p(\\mathbf{z}_k)}{q_{\\gamma}(\\mathbf{z} | \\mathbf{x}^o)} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic case\n",
    "The model we are building has a Gaussian prior and a Gaussian observation model (also the decoder ($z \\rightarrow x$) ),\n",
    "\n",
    "$$ p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z} | \\mathbf{0}, \\mathbf{I})$$\n",
    "\n",
    "$$ p_\\theta(\\mathbf{x} | \\mathbf{z}) = \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}_{\\theta}(\\mathbf{z}), \\sigma^2\\mathbf{I})$$\n",
    "\n",
    "$$ p_\\theta(\\mathbf{x}) = \\int p_\\theta(\\mathbf{x} | \\mathbf{z})p(\\mathbf{z}) d\\mathbf{z}$$\n",
    "\n",
    "where $\\mathbf{\\mu}_{\\theta}(\\mathbf{z}): \\mathbb{R}^d \\rightarrow \\mathbb{R}^p $ in general is a deep neural net, but in this case is a linear mapping, $\\mathbf{\\mu} = \\mathbf{Wz + b}$.\n",
    "\n",
    "The variational posterior (also the encoder ($x \\rightarrow z$) ) is also Gaussian\n",
    "\n",
    "$$q_{\\gamma}(\\mathbf{z} | \\mathbf{x}) = \\mathcal{N}(\\mathbf{z} | \\mu_{\\gamma}(\\mathbf{x}), \\sigma_{\\gamma}(\\mathbf{x})^2 \\mathbf{I})$$\n",
    "\n",
    "If the missing process is *missing at random*, it is ignorable and the ELBO becomes, as described in [the MIWAE paper](https://arxiv.org/abs/1812.02633)\n",
    "\n",
    "$$ E_{\\mathbf{z}_1...\\mathbf{z}_K} \\left[ \\log \\frac{1}{K}\\sum_{k=1}^K \\frac{p_{\\theta}(\\mathbf{x^o} | \\mathbf{z}_k)p(\\mathbf{z}_k)}{q_{\\gamma}(\\mathbf{z}_k | \\mathbf{x^o})} \\right] $$\n",
    "\n",
    "When the missing process is MNAR it is non-ignorable and we need to include the missing model. In this example we include the missing model as a logistic regression in each feature dimension\n",
    "\n",
    "$$ p_{\\phi}(\\mathbf{s} | \\mathbf{x^o, x^m}) = \\text{Bern}(\\mathbf{s} | \\pi_{\\phi}(\\mathbf{x^o, x^m}))$$\n",
    "\n",
    "$$ \\pi_{\\phi, j}(x_j) = \\frac{1}{1 + e^{-\\text{logits}_j}} $$\n",
    "\n",
    "$$ \\text{logits}_j = W_j (x_j - b_j) $$\n",
    "\n",
    "The ELBO in the MNAR case becomes\n",
    "\n",
    "$$ E_{(\\mathbf{z}_1, \\mathbf{x}_1^m)...(\\mathbf{z}_K, \\mathbf{x}_K^m)} \\left[ \\log \\frac{1}{K} \\sum_{k=1}^K \\frac{p_{\\phi}(\\mathbf{s} | \\mathbf{x}^o, \\mathbf{x}_k^m) p_{\\theta}(\\mathbf{x}^o | \\mathbf{z}_k) p(\\mathbf{z}_k)}{q_{\\gamma}(\\mathbf{z} | \\mathbf{x}^o)} \\right]$$\n",
    "\n",
    "with $ z \\sim q_{\\gamma}(z|x^o), x^m\\sim p_\\theta(x^m|z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant to define\n",
    "\n",
    " - $K$ = $n_{\\text{samples}}$ the number of sample to estimate the expectation\n",
    " - $n_{\\text{latent}}$ the dimension of the latent space where $z$ lives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Here we use the white-wine dataset from the UCI database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "data = np.array(pd.read_csv(url, low_memory=False, sep=';'))\n",
    "# ---- drop the classification attribute\n",
    "data = data[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D = data.shape\n",
    "n_latent = D - 1\n",
    "n_hidden = 128\n",
    "n_samples = 20\n",
    "max_iter = 30000\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- standardize data\n",
    "data = data - np.mean(data, axis=0)\n",
    "data = data / np.std(data, axis=0)\n",
    "\n",
    "# ---- random permutation\n",
    "p = np.random.permutation(N)\n",
    "data = data[p, :]\n",
    "\n",
    "# ---- we use the full dataset for training here, but you can make a train-val split\n",
    "Xtrain = data.copy()\n",
    "Xval = Xtrain.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce missing \n",
    "Here we denote\n",
    "- Xnan: data matrix with np.nan as the missing entries\n",
    "- Xz: data matrix with 0 as the missing entries\n",
    "- S: missing mask \n",
    "\n",
    "The missing process depends on the missing data itself:\n",
    "- in half the features, set the feature value to missing when it is higher than the feature mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- introduce missing process\n",
    "Xnan = Xtrain.copy()\n",
    "Xz = Xtrain.copy()\n",
    "\n",
    "mean = np.mean(Xnan[:, :int(D / 2)], axis=0)\n",
    "ix_larger_than_mean = Xnan[:, :int(D / 2)] > mean\n",
    "\n",
    "Xnan[:, :int(D / 2)][ix_larger_than_mean] = np.nan\n",
    "Xz[:, :int(D / 2)][ix_larger_than_mean] = 0\n",
    "\n",
    "S = np.array(~np.isnan(Xnan), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class notMIWAE(nn.Module):\n",
    "    #Only Gaussian and Bern for the moment\n",
    "    def __init__(self, input_size = 10, n_latent = 20, n_samples = 10):\n",
    "        super(notMIWAE, self).__init__()\n",
    "\n",
    "        self.n_input = input_size\n",
    "        self.n_latent = n_latent\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "        self.encoder_mu = nn.Linear(in_features=input_size, out_features=n_latent)\n",
    "        self.encoder_logsigma = nn.Linear(in_features=input_size, out_features=n_latent)\n",
    "        \n",
    "        self.decoder_mu = nn.Linear(in_features=n_latent, out_features=input_size)\n",
    "        self.decoder_logsigma = nn.Linear(in_features=n_latent, out_features=input_size)\n",
    "\n",
    "        # Missing mechanism\n",
    "        self.logits = nn.Linear(in_features=input_size, out_features=input_size)\n",
    "\n",
    "        self.sigma = torch.ones(n_latent)\n",
    "        \n",
    "\n",
    "    def elbo(self, x, s):\n",
    "        \n",
    "        z_mu = self.encoder_mu(x)\n",
    "        z_sigma = torch.exp(self.encoder_logsigma(x))\n",
    "\n",
    "        law_z_given_x= torch.distributions.normal.Normal(loc = z_mu, scale = z_sigma)\n",
    "\n",
    "        z_samples = law_z_given_x.sample((self.n_samples,1)).squeeze()\n",
    "\n",
    "        #check size probably need to transpose\n",
    "        print(z_samples.shape)\n",
    "        log_prob_z_given_x = law_z_given_x.log_prob(z_samples)\n",
    "        print( log_prob_z_given_x.shape)\n",
    "        law_z = torch.distributions.normal.Normal(loc = 0., scale = 1.)\n",
    "\n",
    "        log_prob_z = law_z.log_prob(z_samples)\n",
    "\n",
    "\n",
    "\n",
    "        x_mu = self.decoder_mu(z_samples)\n",
    "        x_sigma = torch.exp(self.decoder_logsigma(z_samples))\n",
    "\n",
    "        law_x_given_z = torch.distributions.normal.Normal(loc = x_mu, scale = x_sigma)\n",
    "\n",
    "        x_samples  = law_x_given_z.sample((self.n_samples,1)).squeeze().transpose(0,1)\n",
    "\n",
    "        log_prob_x_given_z = law_x_given_z.log_prob(x_samples)\n",
    "\n",
    "        \n",
    "        # Missing mechanism\n",
    "        # We recreate the x_sample using the real x we know (x_o) and the x_samples we created from z (x_m).\n",
    "        mixed_x_samples = x_samples * s.unsqueeze(-1) + (x*(1-s)).unsqueeze(-1)\n",
    "        logits = self.logits(mixed_x_samples)\n",
    "\n",
    "        law_s_given_x = torch.distributions.bernoulli.Bernoulli(logits=logits)\n",
    "\n",
    "        log_prob_s_given_x = law_s_given_x.log_prob(mixed_x_samples)\n",
    "        \n",
    "        \n",
    "        log_sum_w = torch.logsumexp(log_prob_s_given_x + log_prob_x_given_z + log_prob_z - log_prob_z_given_x, dim = 1) # dim = ?\n",
    "        log_mean_w = log_sum_w - torch.log(self.n_samples)\n",
    "\n",
    "        return log_mean_w.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100, 20])\n",
      "torch.Size([10, 100, 20])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (100) must match the size of tensor b (11) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ANT\\Documents\\info\\PGM-notMIWAE\\my_file.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ANT/Documents/info/PGM-notMIWAE/my_file.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m X_batch \u001b[39m=\u001b[39m X[i:(i\u001b[39m+\u001b[39mbatch_size)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ANT/Documents/info/PGM-notMIWAE/my_file.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m S_batch \u001b[39m=\u001b[39m S[i:(i\u001b[39m+\u001b[39mbatch_size)]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ANT/Documents/info/PGM-notMIWAE/my_file.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m elbo \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49melbo(X_batch,S_batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ANT/Documents/info/PGM-notMIWAE/my_file.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ANT/Documents/info/PGM-notMIWAE/my_file.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m elbo\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;32mc:\\Users\\ANT\\Documents\\info\\PGM-notMIWAE\\my_file.ipynb Cell 14\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ANT/Documents/info/PGM-notMIWAE/my_file.ipynb#X22sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m log_prob_x_given_z \u001b[39m=\u001b[39m law_x_given_z\u001b[39m.\u001b[39mlog_prob(x_samples)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ANT/Documents/info/PGM-notMIWAE/my_file.ipynb#X22sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# Missing mechanism\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ANT/Documents/info/PGM-notMIWAE/my_file.ipynb#X22sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# We recreate the x_sample using the real x we know (x_o) and the x_samples we created from z (x_m).\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ANT/Documents/info/PGM-notMIWAE/my_file.ipynb#X22sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m mixed_x_samples \u001b[39m=\u001b[39m x_samples \u001b[39m*\u001b[39;49m s\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m+\u001b[39m (x\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39ms))\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ANT/Documents/info/PGM-notMIWAE/my_file.ipynb#X22sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits(mixed_x_samples)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ANT/Documents/info/PGM-notMIWAE/my_file.ipynb#X22sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m law_s_given_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mbernoulli\u001b[39m.\u001b[39mBernoulli(logits\u001b[39m=\u001b[39mlogits)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (100) must match the size of tensor b (11) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "N, p = Xtrain.shape\n",
    "\n",
    "X = torch.Tensor(Xz)\n",
    "S = torch.Tensor(S)\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "model = notMIWAE(input_size=p)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for _ in range(epochs):\n",
    "    for i in range(0,N,batch_size):\n",
    "        X_batch = X[i:(i+batch_size)]\n",
    "        S_batch = S[i:(i+batch_size)]\n",
    "\n",
    "        elbo = model.elbo(X_batch,S_batch)\n",
    "        optimizer.zero_grad()\n",
    "        elbo.backward()\n",
    "        optimizer.step()\n",
    "    print(elbo)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7964,  0.9837, -0.1394,  0.5177, -0.8972],\n",
      "        [ 2.6495, -0.2887,  0.0622, -0.1621,  0.2745],\n",
      "        [ 0.0130,  0.6976,  1.3022,  0.1899, -0.8932]])\n"
     ]
    }
   ],
   "source": [
    "law_z_given_x= torch.distributions.normal.Normal(loc = torch.zeros((3,5)), scale = torch.ones((3,5)))\n",
    "\n",
    "z_samples = law_z_given_x.sample((1,1)).squeeze()\n",
    "\n",
    "#check size probably need to transpose\n",
    "print(z_samples)\n",
    "log_prob_z_given_x = law_z_given_x.log_prob(z_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2360, -1.4028, -0.9287, -1.0529, -1.3214],\n",
      "        [-4.4289, -0.9606, -0.9209, -0.9321, -0.9566],\n",
      "        [-0.9190, -1.1623, -1.7668, -0.9370, -1.3179]])\n"
     ]
    }
   ],
   "source": [
    "print( log_prob_z_given_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2360650132046727"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([ 0.7964,  0.9837, -0.1394,  0.5177, -0.8972])\n",
    "-0.7964**2 / 2 - 0.5 * np.log(2 * np.pi) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
