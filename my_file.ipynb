{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General case:\n",
    "- Choose a prior for $Z$: $p(Z)$.\n",
    "- Choose an observation model: $p_\\theta(X|Z)$\n",
    "- Choose a variational posterior: $q_{\\gamma}(\\mathbf{z} | \\mathbf{x})$\n",
    "\n",
    "- Choose a missing model: $p_{\\phi}(\\mathbf{S} | \\mathbf{X^o, X^m})$\n",
    "\n",
    "\n",
    "The ELBO in the MNAR case is\n",
    "\n",
    "$$ E_{(\\mathbf{z}_1, \\mathbf{x}_1^m)...(\\mathbf{z}_K, \\mathbf{x}_K^m)} \\left[ \\log \\frac{1}{K} \\sum_{k=1}^K \\frac{p_{\\phi}(\\mathbf{s} | \\mathbf{x}^o, \\mathbf{x}_k^m) p_{\\theta}(\\mathbf{x}^o | \\mathbf{z}_k) p(\\mathbf{z}_k)}{q_{\\gamma}(\\mathbf{z} | \\mathbf{x}^o)} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic case\n",
    "The model we are building has a Gaussian prior and a Gaussian observation model (also the decoder ($z \\rightarrow x$) ),\n",
    "\n",
    "$$ p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z} | \\mathbf{0}, \\mathbf{I})$$\n",
    "\n",
    "$$ p_\\theta(\\mathbf{x} | \\mathbf{z}) = \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}_{\\theta}(\\mathbf{z}), \\sigma^2\\mathbf{I})$$\n",
    "\n",
    "$$ p_\\theta(\\mathbf{x}) = \\int p_\\theta(\\mathbf{x} | \\mathbf{z})p(\\mathbf{z}) d\\mathbf{z}$$\n",
    "\n",
    "where $\\mathbf{\\mu}_{\\theta}(\\mathbf{z}): \\mathbb{R}^d \\rightarrow \\mathbb{R}^p $ in general is a deep neural net, but in this case is a linear mapping, $\\mathbf{\\mu} = \\mathbf{Wz + b}$.\n",
    "\n",
    "The variational posterior (also the encoder ($x \\rightarrow z$) ) is also Gaussian\n",
    "\n",
    "$$q_{\\gamma}(\\mathbf{z} | \\mathbf{x}) = \\mathcal{N}(\\mathbf{z} | \\mu_{\\gamma}(\\mathbf{x}), \\sigma_{\\gamma}(\\mathbf{x})^2 \\mathbf{I})$$\n",
    "\n",
    "If the missing process is *missing at random*, it is ignorable and the ELBO becomes, as described in [the MIWAE paper](https://arxiv.org/abs/1812.02633)\n",
    "\n",
    "$$ E_{\\mathbf{z}_1...\\mathbf{z}_K} \\left[ \\log \\frac{1}{K}\\sum_{k=1}^K \\frac{p_{\\theta}(\\mathbf{x^o} | \\mathbf{z}_k)p(\\mathbf{z}_k)}{q_{\\gamma}(\\mathbf{z}_k | \\mathbf{x^o})} \\right] $$\n",
    "\n",
    "When the missing process is MNAR it is non-ignorable and we need to include the missing model. In this example we include the missing model as a logistic regression in each feature dimension\n",
    "\n",
    "$$ p_{\\phi}(\\mathbf{s} | \\mathbf{x^o, x^m}) = \\text{Bern}(\\mathbf{s} | \\pi_{\\phi}(\\mathbf{x^o, x^m}))$$\n",
    "\n",
    "$$ \\pi_{\\phi, j}(x_j) = \\frac{1}{1 + e^{-\\text{logits}_j}} $$\n",
    "\n",
    "$$ \\text{logits}_j = W_j (x_j - b_j) $$\n",
    "\n",
    "The ELBO in the MNAR case becomes\n",
    "\n",
    "$$ E_{(\\mathbf{z}_1, \\mathbf{x}_1^m)...(\\mathbf{z}_K, \\mathbf{x}_K^m)} \\left[ \\log \\frac{1}{K} \\sum_{k=1}^K \\frac{p_{\\phi}(\\mathbf{s} | \\mathbf{x}^o, \\mathbf{x}_k^m) p_{\\theta}(\\mathbf{x}^o | \\mathbf{z}_k) p(\\mathbf{z}_k)}{q_{\\gamma}(\\mathbf{z} | \\mathbf{x}^o)} \\right]$$\n",
    "\n",
    "with $ z \\sim q_{\\gamma}(z|x^o), x^m\\sim p_\\theta(x^m|z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant to define\n",
    "\n",
    " - $K$ = $n_{\\text{samples}}$ the number of sample to estimate the expectation\n",
    " - $n_{\\text{latent}}$ the dimension of the latent space where $z$ lives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Here we use the white-wine dataset from the UCI database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "data = np.array(pd.read_csv(url, low_memory=False, sep=';'))\n",
    "# ---- drop the classification attribute\n",
    "data = data[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D = data.shape\n",
    "n_latent = D - 1\n",
    "n_hidden = 128\n",
    "n_samples = 20\n",
    "max_iter = 30000\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- standardize data\n",
    "data = data - np.mean(data, axis=0)\n",
    "data = data / np.std(data, axis=0)\n",
    "\n",
    "# ---- random permutation\n",
    "p = np.random.permutation(N)\n",
    "data = data[p, :]\n",
    "\n",
    "# ---- we use the full dataset for training here, but you can make a train-val split\n",
    "Xtrain = data.copy()\n",
    "Xval = Xtrain.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce missing \n",
    "Here we denote\n",
    "- Xnan: data matrix with np.nan as the missing entries\n",
    "- Xz: data matrix with 0 as the missing entries\n",
    "- S: missing mask \n",
    "\n",
    "The missing process depends on the missing data itself:\n",
    "- in half the features, set the feature value to missing when it is higher than the feature mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- introduce missing process\n",
    "Xnan = Xtrain.copy()\n",
    "Xz = Xtrain.copy()\n",
    "\n",
    "mean = np.mean(Xnan[:, :int(D / 2)], axis=0)\n",
    "ix_larger_than_mean = Xnan[:, :int(D / 2)] > mean\n",
    "\n",
    "Xnan[:, :int(D / 2)][ix_larger_than_mean] = np.nan\n",
    "Xz[:, :int(D / 2)][ix_larger_than_mean] = 0\n",
    "\n",
    "S = np.array(~np.isnan(Xnan), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(tens,name=None):\n",
    "    if torch.isnan(tens).any().item():\n",
    "        print(name)\n",
    "        print(tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clip(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.clip(x,-10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class notMIWAE(nn.Module):\n",
    "    #Only Gaussian and Bern for the moment\n",
    "    def __init__(self, input_size = 10, n_latent = 20, n_samples = 10):\n",
    "        super(notMIWAE, self).__init__()\n",
    "\n",
    "        self.n_input = input_size\n",
    "        self.n_latent = n_latent\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "        self.encoder_mu = nn.Linear(in_features=input_size, out_features=n_latent)\n",
    "        self.encoder_logsigma = nn.Sequential(nn.Linear(in_features=input_size, out_features=n_latent),Clip())\n",
    "        \n",
    "        self.decoder_mu = nn.Linear(in_features=n_latent, out_features=input_size)\n",
    "        # self.decoder_logsigma = nn.Linear(in_features=n_latent, out_features=input_size)\n",
    "\n",
    "        # Missing mechanism\n",
    "        self.logits = nn.Linear(in_features=input_size, out_features=input_size)\n",
    "\n",
    "        self.sigma = torch.ones(n_latent)\n",
    "        \n",
    "\n",
    "    def elbo(self, x, s):\n",
    "        \"\"\"\n",
    "        x : the input of size (batch, input_size)\n",
    "        s : the mask of size (batch, input_size) s[i,j] = 1 if x[i,j] exists else 0\n",
    "        \"\"\"\n",
    "        \n",
    "        z_mu = self.encoder_mu(x) # (batch, n_latent)\n",
    "        z_sigma = torch.sqrt(torch.exp(self.encoder_logsigma(x))) # (batch, n_latent)\n",
    "       \n",
    "        \n",
    "        law_z_given_x= torch.distributions.normal.Normal(loc = z_mu, scale = z_sigma) # Distribution with parameter of size (batch, n_latent)\n",
    "\n",
    "        z_samples = law_z_given_x.sample((self.n_samples,1)).squeeze() # (n_samples, batch, n_latent)\n",
    "\n",
    "        log_prob_z_given_x = law_z_given_x.log_prob(z_samples).sum(dim=-1) # (n_samples, batch)\n",
    "        \n",
    "        \n",
    "        z_samples = z_samples.transpose(0,1) # (batch, n_samples, n_latent)\n",
    "        log_prob_z_given_x = log_prob_z_given_x.transpose(0,1) # (batch, n_samples)\n",
    "\n",
    "        law_z = torch.distributions.normal.Normal(loc = 0., scale = 1.) \n",
    "\n",
    "        log_prob_z = law_z.log_prob(z_samples).sum(dim=-1) # (batch, n_samples)\n",
    "\n",
    "        x_mu = self.decoder_mu(z_samples) # (batch, n_samples, input_size)\n",
    "\n",
    "        x_sigma = 1 # torch.sqrt(torch.exp(self.decoder_logsigma(z_samples) + 1e-5)) # (batch, n_samples, input_size)\n",
    "\n",
    "        law_x_given_z = torch.distributions.normal.Normal(loc = x_mu, scale = x_sigma) # Distribution with parameter of size (batch, n_samples, input_size)\n",
    "\n",
    "        x_samples  = law_x_given_z.sample().squeeze() # (batch, n_samples, input_size)\n",
    "\n",
    "        log_prob_x_given_z = (law_x_given_z.log_prob(x.unsqueeze(1)) * s.unsqueeze(1)).sum(dim=-1) # (batch, n_samples)\n",
    "\n",
    "\n",
    "        \n",
    "        # Missing mechanism\n",
    "        \n",
    "        # We recreate the x_sample using the real x we know (x_o) and the x_samples we created from z (x_m).\n",
    "        mixed_x_samples = x_samples * (1-s).unsqueeze(1) + (x*s).unsqueeze(1) # (batch, n_samples, input_size)\n",
    "\n",
    "        logits = self.logits(mixed_x_samples) # (batch, n_samples, input_size)\n",
    "\n",
    "        law_s_given_x = torch.distributions.bernoulli.Bernoulli(logits=logits) # Distribution with parameter of size (batch, n_samples, input_size)\n",
    "\n",
    "        log_prob_s_given_x = law_s_given_x.log_prob(s.unsqueeze(1)).sum(dim=-1) # (batch, n_samples)\n",
    "\n",
    "\n",
    "        log_sum_w = torch.logsumexp(log_prob_s_given_x + log_prob_x_given_z + log_prob_z - log_prob_z_given_x, dim = 1) # (batch)\n",
    "        log_mean_w = log_sum_w - torch.log(torch.Tensor([self.n_samples])) # (batch)\n",
    "\n",
    "        \n",
    "        return  - log_mean_w.mean()\n",
    "    # law_z_given_x2 = torch.distributions.normal.Normal(loc=z_mu.unsqueeze(0), scale=z_sigma.unsqueeze(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:1\n",
      "loss 22.115385055541992\n",
      "Epochs:2\n",
      "loss 22.674625396728516\n",
      "Epochs:3\n",
      "loss 23.7463321685791\n",
      "Epochs:4\n",
      "loss 26.109050750732422\n",
      "Epochs:5\n",
      "loss 29.575510025024414\n",
      "Epochs:6\n",
      "loss 33.78615188598633\n",
      "Epochs:7\n",
      "loss 38.96272277832031\n",
      "Epochs:8\n",
      "loss 46.23461151123047\n",
      "Epochs:9\n",
      "loss 54.965816497802734\n",
      "Epochs:10\n",
      "loss 67.98766326904297\n",
      "Epochs:11\n",
      "loss 80.99969482421875\n",
      "Epochs:12\n",
      "loss 110.55052185058594\n",
      "Epochs:13\n",
      "loss 144.79624938964844\n",
      "Epochs:14\n",
      "loss 184.99928283691406\n",
      "Epochs:15\n",
      "loss 259.9579772949219\n",
      "Epochs:16\n",
      "loss 359.18603515625\n",
      "Epochs:17\n",
      "loss 465.5513916015625\n",
      "Epochs:18\n",
      "loss 610.2247924804688\n",
      "Epochs:19\n",
      "loss 785.813232421875\n",
      "Epochs:20\n",
      "loss 979.40380859375\n"
     ]
    }
   ],
   "source": [
    "N, p = Xtrain.shape\n",
    "Xz[np.isnan(Xz)] = 0\n",
    "X = torch.FloatTensor(Xz)\n",
    "S = torch.FloatTensor(S)\n",
    "batch_size = 100\n",
    "epochs = 20\n",
    "model = notMIWAE(input_size=p,n_samples=20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epochs:{epoch+1}')\n",
    "    p = np.random.permutation(N)\n",
    "    X = X[p,:]\n",
    "    S = S[p,:]\n",
    "    \n",
    "    for i in range(0,N,batch_size):\n",
    "        print(f'{i+batch_size} / {N}', end=\"\\r\")\n",
    "        X_batch = X[i:(i+batch_size)]\n",
    "        S_batch = S[i:(i+batch_size)]\n",
    "\n",
    "        if torch.isnan(X_batch).any().item():\n",
    "                print('NaN X_batch')\n",
    "        if torch.isnan(S_batch).any().item():\n",
    "                print('NaN S_batch')\n",
    "                \n",
    "        elbo = model.elbo(X_batch,S_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        elbo.backward()\n",
    "        optimizer.step()\n",
    "        for param in model.parameters():\n",
    "            if torch.isnan(param).any().item():\n",
    "                print('NaN parameter')\n",
    "                print(torch.isnan(param).any().item())\n",
    "    \n",
    "    print('loss', model.elbo(X,S).item())\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(torch.isnan(param).any().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2922, -0.0183, -1.3425,  1.0649, -0.1271],\n",
      "        [ 1.2910,  1.3198,  1.2126, -0.2548,  0.2058],\n",
      "        [ 2.7040, -0.5960,  0.5669, -0.9396,  0.8833]])\n"
     ]
    }
   ],
   "source": [
    "law_z_given_x= torch.distributions.normal.Normal(loc = torch.zeros((3,5)), scale = torch.ones((3,5)))\n",
    "\n",
    "z_samples = law_z_given_x.sample((1,1)).squeeze()\n",
    "\n",
    "#check size probably need to transpose\n",
    "print(z_samples)\n",
    "log_prob_z_given_x = law_z_given_x.log_prob(z_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9616, -0.9191, -1.8201, -1.4859, -0.9270],\n",
      "        [-1.7522, -1.7899, -1.6541, -0.9514, -0.9401],\n",
      "        [-4.5748, -1.0965, -1.0796, -1.3603, -1.3090]])\n"
     ]
    }
   ],
   "source": [
    "print( log_prob_z_given_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2360650132046727"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([ 0.7964,  0.9837, -0.1394,  0.5177, -0.8972])\n",
    "-0.7964**2 / 2 - 0.5 * np.log(2 * np.pi) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
