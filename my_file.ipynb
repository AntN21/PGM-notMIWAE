{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General case:\n",
    "- Choose a prior for $Z$: $p(Z)$.\n",
    "- Choose an observation model: $p_\\theta(X|Z)$\n",
    "- Choose a variational posterior: $q_{\\gamma}(\\mathbf{z} | \\mathbf{x})$\n",
    "\n",
    "- Choose a missing model: $p_{\\phi}(\\mathbf{S} | \\mathbf{X^o, X^m})$\n",
    "\n",
    "\n",
    "The ELBO in the MNAR case is\n",
    "\n",
    "$$ E_{(\\mathbf{z}_1, \\mathbf{x}_1^m)...(\\mathbf{z}_K, \\mathbf{x}_K^m)} \\left[ \\log \\frac{1}{K} \\sum_{k=1}^K \\frac{p_{\\phi}(\\mathbf{s} | \\mathbf{x}^o, \\mathbf{x}_k^m) p_{\\theta}(\\mathbf{x}^o | \\mathbf{z}_k) p(\\mathbf{z}_k)}{q_{\\gamma}(\\mathbf{z} | \\mathbf{x}^o)} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic case\n",
    "The model we are building has a Gaussian prior and a Gaussian observation model (also the decoder ($z \\rightarrow x$) ),\n",
    "\n",
    "$$ p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z} | \\mathbf{0}, \\mathbf{I})$$\n",
    "\n",
    "$$ p_\\theta(\\mathbf{x} | \\mathbf{z}) = \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}_{\\theta}(\\mathbf{z}), \\sigma^2\\mathbf{I})$$\n",
    "\n",
    "$$ p_\\theta(\\mathbf{x}) = \\int p_\\theta(\\mathbf{x} | \\mathbf{z})p(\\mathbf{z}) d\\mathbf{z}$$\n",
    "\n",
    "where $\\mathbf{\\mu}_{\\theta}(\\mathbf{z}): \\mathbb{R}^d \\rightarrow \\mathbb{R}^p $ in general is a deep neural net, but in this case is a linear mapping, $\\mathbf{\\mu} = \\mathbf{Wz + b}$.\n",
    "\n",
    "The variational posterior (also the encoder ($x \\rightarrow z$) ) is also Gaussian\n",
    "\n",
    "$$q_{\\gamma}(\\mathbf{z} | \\mathbf{x}) = \\mathcal{N}(\\mathbf{z} | \\mu_{\\gamma}(\\mathbf{x}), \\sigma_{\\gamma}(\\mathbf{x})^2 \\mathbf{I})$$\n",
    "\n",
    "If the missing process is *missing at random*, it is ignorable and the ELBO becomes, as described in [the MIWAE paper](https://arxiv.org/abs/1812.02633)\n",
    "\n",
    "$$ E_{\\mathbf{z}_1...\\mathbf{z}_K} \\left[ \\log \\frac{1}{K}\\sum_{k=1}^K \\frac{p_{\\theta}(\\mathbf{x^o} | \\mathbf{z}_k)p(\\mathbf{z}_k)}{q_{\\gamma}(\\mathbf{z}_k | \\mathbf{x^o})} \\right] $$\n",
    "\n",
    "When the missing process is MNAR it is non-ignorable and we need to include the missing model. In this example we include the missing model as a logistic regression in each feature dimension\n",
    "\n",
    "$$ p_{\\phi}(\\mathbf{s} | \\mathbf{x^o, x^m}) = \\text{Bern}(\\mathbf{s} | \\pi_{\\phi}(\\mathbf{x^o, x^m}))$$\n",
    "\n",
    "$$ \\pi_{\\phi, j}(x_j) = \\frac{1}{1 + e^{-\\text{logits}_j}} $$\n",
    "\n",
    "$$ \\text{logits}_j = W_j (x_j - b_j) $$\n",
    "\n",
    "The ELBO in the MNAR case becomes\n",
    "\n",
    "$$ E_{(\\mathbf{z}_1, \\mathbf{x}_1^m)...(\\mathbf{z}_K, \\mathbf{x}_K^m)} \\left[ \\log \\frac{1}{K} \\sum_{k=1}^K \\frac{p_{\\phi}(\\mathbf{s} | \\mathbf{x}^o, \\mathbf{x}_k^m) p_{\\theta}(\\mathbf{x}^o | \\mathbf{z}_k) p(\\mathbf{z}_k)}{q_{\\gamma}(\\mathbf{z} | \\mathbf{x}^o)} \\right]$$\n",
    "\n",
    "with $ z \\sim q_{\\gamma}(z|x^o), x^m\\sim p_\\theta(x^m|z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant to define\n",
    "\n",
    " - $K$ = $n_{\\text{samples}}$ the number of sample to estimate the expectation\n",
    " - $n_{\\text{latent}}$ the dimension of the latent space where $z$ lives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Here we use the white-wine dataset from the UCI database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "data = np.array(pd.read_csv(url, low_memory=False, sep=';'))\n",
    "# ---- drop the classification attribute\n",
    "data = data[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D = data.shape\n",
    "n_latent = D - 1\n",
    "n_hidden = 128\n",
    "n_samples = 20\n",
    "max_iter = 30000\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- standardize data\n",
    "data = data - np.mean(data, axis=0)\n",
    "data = data / np.std(data, axis=0)\n",
    "\n",
    "# ---- random permutation\n",
    "p = np.random.permutation(N)\n",
    "data = data[p, :]\n",
    "\n",
    "# ---- we use the full dataset for training here, but you can make a train-val split\n",
    "Xtrain = data.copy()\n",
    "Xval = Xtrain.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce missing \n",
    "Here we denote\n",
    "- Xnan: data matrix with np.nan as the missing entries\n",
    "- Xz: data matrix with 0 as the missing entries\n",
    "- S: missing mask \n",
    "\n",
    "The missing process depends on the missing data itself:\n",
    "- in half the features, set the feature value to missing when it is higher than the feature mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- introduce missing process\n",
    "Xnan = Xtrain.copy()\n",
    "Xz = Xtrain.copy()\n",
    "\n",
    "mean = np.mean(Xnan[:, :int(D / 2)], axis=0)\n",
    "ix_larger_than_mean = Xnan[:, :int(D / 2)] > mean\n",
    "\n",
    "Xnan[:, :int(D / 2)][ix_larger_than_mean] = np.nan\n",
    "Xz[:, :int(D / 2)][ix_larger_than_mean] = 0\n",
    "\n",
    "S = np.array(~np.isnan(Xnan), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(tens,name=None):\n",
    "    if torch.isnan(tens).any().item():\n",
    "        print(name)\n",
    "        print(tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clip(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.clip(x,-10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distribution():\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        pass\n",
    "    def rsample(self, sample_shape):\n",
    "        print(\"Not implemented\")\n",
    "        pass\n",
    "    def log_prob(self, value):\n",
    "        print(\"Not implemented\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m      2\u001b[0m a\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([1,2,3])\n",
    "a\n",
    "torch.FloatTensor([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussDistribution(Distribution):\n",
    "    def __init__(self, loc: Any = 0., scale: Any = 1.) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mu = torch.tensor(loc, dtype=torch.float32)\n",
    "        self.sigma = torch.tensor(scale, dtype=torch.float32)\n",
    "\n",
    "    def rsample(self, sample_shape):\n",
    "        samples = self.mu + self.sigma * torch.randn(sample_shape, dtype=self.mu.dtype)\n",
    "        return samples\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        eps = torch.finfo(torch.float32).eps\n",
    "\n",
    "        log_p = - 0.5 * torch.log(2 * np.pi) - 0.5 * torch.log(self.sigma**2) \\\n",
    "                      - 0.5 * torch.square(value - self.mu) / (self.sigma**2 + eps)\n",
    "        return log_p \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class notMIWAE(nn.Module):\n",
    "    #Only Gaussian and Bern for the moment\n",
    "    def __init__(self, input_size = 10, n_latent = 20, n_samples = 10):\n",
    "        super(notMIWAE, self).__init__()\n",
    "\n",
    "        self.n_input = input_size\n",
    "        self.n_latent = n_latent\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "        self.encoder_mu = nn.Linear(in_features=input_size, out_features=n_latent)\n",
    "        self.encoder_logsigma = nn.Sequential(nn.Linear(in_features=input_size, out_features=n_latent),Clip())\n",
    "        \n",
    "        self.decoder_mu = nn.Linear(in_features=n_latent, out_features=input_size)\n",
    "        # self.decoder_logsigma = nn.Linear(in_features=n_latent, out_features=input_size)\n",
    "\n",
    "        # Missing mechanism\n",
    "        self.logits = nn.Linear(in_features=input_size, out_features=input_size)\n",
    "\n",
    "        self.sigma = torch.ones(n_latent)\n",
    "\n",
    "        self.prior = GaussDistribution(loc = 0., scale = 1.) # torch.distributions.normal.Normal(loc = 0., scale = 1.)\n",
    "        \n",
    "\n",
    "    def elbo(self, x, s):\n",
    "        \"\"\"\n",
    "        x : the input of size (batch, input_size)\n",
    "        s : the mask of size (batch, input_size) s[i,j] = 1 if x[i,j] exists else 0\n",
    "        \"\"\"\n",
    "        \n",
    "        z_mu = self.encoder_mu(x) # (batch, n_latent)\n",
    "        z_sigma = torch.sqrt(torch.exp(self.encoder_logsigma(x))) # (batch, n_latent)\n",
    "       \n",
    "        \n",
    "        law_z_given_x= torch.distributions.normal.Normal(loc = z_mu, scale = z_sigma) # Distribution with parameter of size (batch, n_latent)\n",
    "\n",
    "        z_samples = law_z_given_x.rsample((self.n_samples,1)).squeeze() # (n_samples, batch, n_latent)\n",
    "\n",
    "        log_prob_z_given_x = law_z_given_x.log_prob(z_samples).sum(dim=-1) # (n_samples, batch)\n",
    "        \n",
    "        \n",
    "        z_samples = z_samples.transpose(0,1) # (batch, n_samples, n_latent)\n",
    "        log_prob_z_given_x = log_prob_z_given_x.transpose(0,1) # (batch, n_samples)\n",
    "\n",
    "        law_z = self.prior\n",
    "\n",
    "        log_prob_z = law_z.log_prob(z_samples).sum(dim=-1) # (batch, n_samples)\n",
    "\n",
    "        x_mu = self.decoder_mu(z_samples) # (batch, n_samples, input_size)\n",
    "\n",
    "        x_sigma = 1 # torch.sqrt(torch.exp(self.decoder_logsigma(z_samples) + 1e-5)) # (batch, n_samples, input_size)\n",
    "\n",
    "        law_x_given_z = torch.distributions.normal.Normal(loc = x_mu, scale = x_sigma) # Distribution with parameter of size (batch, n_samples, input_size)\n",
    "\n",
    "        x_samples  = law_x_given_z.rsample().squeeze() # (batch, n_samples, input_size)\n",
    "\n",
    "        log_prob_x_given_z = (law_x_given_z.log_prob(x.unsqueeze(1)) * s.unsqueeze(1)).sum(dim=-1) # (batch, n_samples)\n",
    "\n",
    "\n",
    "        \n",
    "        # Missing mechanism\n",
    "        \n",
    "        # We recreate the x_sample using the real x we know (x_o) and the x_samples we created from z (x_m).\n",
    "        mixed_x_samples = x_samples * (1-s).unsqueeze(1) + (x*s).unsqueeze(1) # (batch, n_samples, input_size)\n",
    "\n",
    "        logits = self.logits(mixed_x_samples) # (batch, n_samples, input_size)\n",
    "\n",
    "        law_s_given_x = torch.distributions.bernoulli.Bernoulli(logits=logits) # Distribution with parameter of size (batch, n_samples, input_size)\n",
    "\n",
    "        log_prob_s_given_x = law_s_given_x.log_prob(s.unsqueeze(1)).sum(dim=-1) # (batch, n_samples)\n",
    "\n",
    "\n",
    "        log_sum_w = torch.logsumexp(log_prob_s_given_x + log_prob_x_given_z + log_prob_z - log_prob_z_given_x, dim = 1) # (batch)\n",
    "        log_mean_w = log_sum_w - torch.log(torch.Tensor([self.n_samples])) # (batch)\n",
    "\n",
    "        \n",
    "        return  - log_mean_w.mean()\n",
    "    # law_z_given_x2 = torch.distributions.normal.Normal(loc=z_mu.unsqueeze(0), scale=z_sigma.unsqueeze(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): data must be a sequence (got float)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      5\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mnotMIWAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "Cell \u001b[1;32mIn[26], line 21\u001b[0m, in \u001b[0;36mnotMIWAE.__init__\u001b[1;34m(self, input_size, n_latent, n_samples)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39minput_size, out_features\u001b[38;5;241m=\u001b[39minput_size)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(n_latent)\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior \u001b[38;5;241m=\u001b[39m \u001b[43mGaussDistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m, in \u001b[0;36mGaussDistribution.__init__\u001b[1;34m(self, loc, scale)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, loc: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m, scale: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(scale)\n",
      "\u001b[1;31mTypeError\u001b[0m: new(): data must be a sequence (got float)"
     ]
    }
   ],
   "source": [
    "N, p = Xtrain.shape\n",
    "X = torch.FloatTensor(Xz)\n",
    "S = torch.FloatTensor(S)\n",
    "batch_size = 100\n",
    "epochs = 20\n",
    "model = notMIWAE(input_size=p,n_samples=20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epochs:{epoch+1}')\n",
    "    p = np.random.permutation(N)\n",
    "    X = X[p,:]\n",
    "    S = S[p,:]\n",
    "    \n",
    "    for i in range(0,N,batch_size):\n",
    "        print(f'{i+batch_size} / {N}', end=\"\\r\")\n",
    "        X_batch = X[i:(i+batch_size)]\n",
    "        S_batch = S[i:(i+batch_size)]\n",
    "\n",
    "        if torch.isnan(X_batch).any().item():\n",
    "                print('NaN X_batch')\n",
    "        if torch.isnan(S_batch).any().item():\n",
    "                print('NaN S_batch')\n",
    "                \n",
    "        elbo = model.elbo(X_batch,S_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        elbo.backward()\n",
    "        optimizer.step()\n",
    "        for param in model.parameters():\n",
    "            if torch.isnan(param).any().item():\n",
    "                print('NaN parameter')\n",
    "                print(torch.isnan(param).any().item())\n",
    "    \n",
    "    print('loss', model.elbo(X,S).item())\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(torch.isnan(param).any().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0074, -0.2796,  0.6426,  1.1826, -1.0374],\n",
      "        [-0.5127, -0.9160, -0.4352, -1.6984, -1.0139],\n",
      "        [-1.3056, -1.3189, -0.7794, -1.4175,  0.3863]])\n"
     ]
    }
   ],
   "source": [
    "law_z_given_x= torch.distributions.normal.Normal(loc = torch.zeros((3,5)), scale = torch.ones((3,5)))\n",
    "\n",
    "z_samples = law_z_given_x.sample((1,1)).squeeze()\n",
    "\n",
    "#check size probably need to transpose\n",
    "print(z_samples)\n",
    "log_prob_z_given_x = law_z_given_x.log_prob(z_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9190, -0.9580, -1.1254, -1.6182, -1.4571],\n",
      "        [-1.0504, -1.3385, -1.0136, -2.3611, -1.4330],\n",
      "        [-1.7712, -1.7886, -1.2227, -1.9236, -0.9936]])\n"
     ]
    }
   ],
   "source": [
    "print( log_prob_z_given_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2360650132046727"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([ 0.7964,  0.9837, -0.1394,  0.5177, -0.8972])\n",
    "-0.7964**2 / 2 - 0.5 * np.log(2 * np.pi) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
